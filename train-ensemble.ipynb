{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import awkward0 as awkward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 0 # random seed corresponds to the member of the ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "tf.config.set_visible_devices([], 'GPU') # disabling GPU, comment out if you want to use GPU\n",
    "\n",
    "# uncomment and adjust for GPU calculations\n",
    "# os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\"\n",
    "\n",
    "# # dynamic memory growth\n",
    "# physical_devices = tf.config.list_physical_devices('GPU')\n",
    "# tf.config.experimental.set_memory_growth(physical_devices[0], True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "from os.path import join\n",
    "#from ROOT import TLorentzVector\n",
    "from tensorflow import keras\n",
    "from keras.optimizers import *\n",
    "from keras.layers import *\n",
    "from keras import regularizers\n",
    "#import seaborn as sns\n",
    "from keras.utils import *\n",
    "import shutil\n",
    "from dataset import *\n",
    "import datetime\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(SEED)\n",
    "np.random.seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NO_OF_PARTICLES = 250 # how many particles in an event to use, default 250\n",
    "PT_CUT = 1.0 # Lower cut on PT particles, default is 1.0\n",
    "OUTPUT = f'model-{SEED}' # name of the output folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(OUTPUT):\n",
    "  # Create a new directory because it does not exist \n",
    "  os.makedirs(OUTPUT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "logging.basicConfig(filename=join(OUTPUT, 'training.log'),\n",
    "                    filemode='a',\n",
    "                    format='%(asctime)s,%(msecs)d %(name)s %(levelname)s %(message)s',\n",
    "                    datefmt='%H:%M:%S',\n",
    "                    level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shutil.copy('train-ensemble.ipynb', join(OUTPUT, 'train-ensemble.ipynb')) # copy notebook to the output dir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load validation set. Adjust the data path if needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_dataset = Dataset(f'data/ensemble_val_{SEED}.awkd', {}, data_format='channel_last', simple_mode=False, pad_len=NO_OF_PARTICLES)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load training set. Adjust the data path if needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = Dataset(f'data/ensemble_train_{SEED}.awkd', {}, data_format='channel_last', simple_mode=False, pad_len=NO_OF_PARTICLES)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate means and standard deviations, and normalise the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scalers = train_dataset.normalize_all(scalers = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_dataset.normalize_all(scalers=scalers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write scalers to file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "string = ''\n",
    "for k,v in scalers.items():\n",
    "    string += k + '\\n'\n",
    "    string += f'mean: {v.mean_[0]}'+ '\\n'\n",
    "    string += f'var: {v.var_[0]}' + '\\n'\n",
    "    string += f'scale: {v.scale_[0]}' + '\\n'\n",
    "print(string)\n",
    "with open(join(OUTPUT, 'scaler.txt'), 'w') as f:\n",
    "    f.write(string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apply PT cut"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_pt_cut(data, cut=PT_CUT):\n",
    "    cond = data.X['mask'] < np.log(cut)\n",
    "    cond2 = data.X['mask'] >= np.log(cut)\n",
    "    print(cond.shape)\n",
    "    print('below the cut: ', np.sum(cond))\n",
    "    print('above the cut: ', np.sum(cond2))\n",
    "    print('sum: ', np.sum(cond) + np.sum(cond2))\n",
    "    data.X['mask'][cond] = 0.0\n",
    "    ext_shape = list(cond.shape)\n",
    "    ext_shape[-1] = 4\n",
    "    new_cond = np.repeat(cond, 5, axis=2)\n",
    "    print(new_cond.shape)\n",
    "    data.X['features'][new_cond] = 0.0\n",
    "  \n",
    "    return data, np.sum(cond2, axis=1).flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset, _ = apply_pt_cut(train_dataset, PT_CUT)\n",
    "val_dataset, _ = apply_pt_cut(val_dataset, PT_CUT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the ParticleNet model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import tf_keras_model\n",
    "\n",
    "model_type = 'particle_net_lite' # choose between 'particle_net' and 'particle_net_lite'\n",
    "num_classes = train_dataset.y.shape[1]\n",
    "input_shapes = {k:train_dataset[k].shape[1:] for k in train_dataset.X}\n",
    "if 'lite' in model_type:\n",
    "    model = tf_keras_model.get_particle_net_lite(num_classes, input_shapes)\n",
    "else:\n",
    "    model = tf_keras_model.get_particle_net(num_classes, input_shapes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a dense network for high-level variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2_inputs = keras.Input(shape=(5+4*4,))\n",
    "xx = model2_inputs\n",
    "xx = keras.layers.Dense(256, activation=None, kernel_regularizer=keras.regularizers.L2(1e-3))(xx)\n",
    "xx = keras.layers.BatchNormalization()(xx)\n",
    "xx = keras.layers.Activation(tf.nn.relu)(xx)\n",
    "for ii in range(5):\n",
    "    xx = keras.layers.Dense(128, activation=None, kernel_regularizer=keras.regularizers.L2(1e-3))(xx)\n",
    "    xx = keras.layers.BatchNormalization()(xx)\n",
    "    xx = keras.layers.Activation(tf.nn.relu)(xx)    \n",
    "xx = keras.layers.Dropout(0.5)(xx)\n",
    "xx = keras.layers.Dense(2, activation=\"softmax\",)(xx)\n",
    "\n",
    "model2 = keras.Model(model2_inputs, xx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2.compile(loss='binary_crossentropy',\n",
    "              optimizer=keras.optimizers.Adam(learning_rate=5e-4), #lr_scheduler),\n",
    "              metrics = [tf.keras.metrics.BinaryAccuracy()],\n",
    "              weighted_metrics=[tf.keras.metrics.AUC()])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modify the GNN part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Improved particle net\n",
    "yy = model.layers[-4].output\n",
    "yy = keras.layers.Dense(256, activation=None, kernel_regularizer=keras.regularizers.L2(1e-3))(yy)\n",
    "yy = keras.layers.BatchNormalization()(yy)\n",
    "yy = keras.layers.Activation(tf.nn.relu)(yy)\n",
    "yy = keras.layers.Dropout(0.5)(yy)\n",
    "yy = keras.layers.Dense(128, activation=None, kernel_regularizer=keras.regularizers.L2(1e-3))(yy)\n",
    "yy = keras.layers.BatchNormalization()(yy)\n",
    "yy = keras.layers.Activation(tf.nn.relu)(yy)\n",
    "\n",
    "model3 = keras.Model(model.input, yy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merge two models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_layer = keras.layers.Concatenate()([model2.layers[-3].output, model3.layers[-1].output])\n",
    "z = merged_layer\n",
    "z = keras.layers.Dense(128, activation=None, kernel_regularizer=keras.regularizers.L2(1e-3))(z)\n",
    "z = keras.layers.BatchNormalization()(z)\n",
    "z = keras.layers.Activation(tf.nn.relu)(z)\n",
    "z = keras.layers.Dropout(0.5)(z)\n",
    "z = keras.layers.Dense(2, activation=\"softmax\",)(z)\n",
    "merged_model = keras.Model([model2.input, model3.input], z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set the batch size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1024"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Warmup cosine decay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cosine_decay import *\n",
    "lr_schedule = WarmUpCosineDecay(start_lr=0, target_lr=5e-4, warmup_steps=10*800, total_steps=310*800, hold=0, final_lr=1e-5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "merged_model.compile(loss='binary_crossentropy',\n",
    "                     optimizer=keras.optimizers.Adam(learning_rate=lr_schedule, clipnorm=5e-4),\n",
    "                     metrics = [tf.keras.metrics.BinaryAccuracy()],\n",
    "                     weighted_metrics=[tf.keras.metrics.AUC()])\n",
    "merged_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare model checkpoint directory.\n",
    "import os\n",
    "save_dir = join(OUTPUT, 'model_checkpoints')\n",
    "model_name = 'model.{epoch:03d}.h5'\n",
    "if not os.path.isdir(save_dir):\n",
    "    os.makedirs(save_dir)\n",
    "filepath = os.path.join(save_dir, model_name)\n",
    "\n",
    "# Prepare callbacks for model saving and for learning rate adjustment.\n",
    "checkpoint = keras.callbacks.ModelCheckpoint(filepath=filepath,\n",
    "                             monitor='val_loss',\n",
    "                             verbose=1,\n",
    "                             save_best_only=True)\n",
    "\n",
    "progress_bar = keras.callbacks.ProgbarLogger()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define early stopping\n",
    "earlystop = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor=\"val_loss\",\n",
    "    min_delta=0,\n",
    "    patience=50,\n",
    "    verbose=1,\n",
    "    mode=\"auto\",\n",
    "    baseline=None,\n",
    "    restore_best_weights=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "callbacks = [checkpoint, progress_bar, earlystop,]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate weights\n",
    "bkg_n = np.sum(train_dataset.y[:,1] == 1.0)\n",
    "sig_n = np.sum(train_dataset.y[:,0] == 1.0)\n",
    "total = bkg_n + sig_n\n",
    "print(bkg_n, sig_n, total)\n",
    "weight_for_bkg = (1 / bkg_n) * (total / 2.0)\n",
    "weight_for_sig = (1 / sig_n) * (total / 2.0)\n",
    "class_weights = {0: weight_for_sig, 1: weight_for_bkg}\n",
    "print(class_weights) # dataset is very well balanced but we use weights anyway"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset.shuffle()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Handle high-level data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "curr_dataset = train_dataset\n",
    "dnn_vars = [curr_dataset['event_met'], curr_dataset['event_ht'], curr_dataset['event_eta'], curr_dataset['event_m'], curr_dataset['event_MT2'], ]\n",
    "for ii in range(0, 4):\n",
    "    dnn_vars.append(curr_dataset.X['jet'][:, ii, 0])\n",
    "    dnn_vars.append(curr_dataset.X['jet'][:, ii, 1])\n",
    "    dnn_vars.append(curr_dataset.X['jet'][:, ii, 3])\n",
    "    dnn_vars.append(curr_dataset['Dphi'][:, ii])\n",
    "model2_train_X = list(zip(*dnn_vars))\n",
    "model2_train_X = np.array(model2_train_X)\n",
    "model2_train_X.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "curr_dataset = val_dataset\n",
    "dnn_vars = [curr_dataset['event_met'], curr_dataset['event_ht'], curr_dataset['event_eta'], curr_dataset['event_m'], curr_dataset['event_MT2'], ]\n",
    "for ii in range(4):\n",
    "    dnn_vars.append(curr_dataset.X['jet'][:, ii, 0])\n",
    "    dnn_vars.append(curr_dataset.X['jet'][:, ii, 1])\n",
    "    dnn_vars.append(curr_dataset.X['jet'][:, ii, 3])\n",
    "    dnn_vars.append(curr_dataset['Dphi'][:, ii])\n",
    "\n",
    "model2_val_X = list(zip(*dnn_vars))\n",
    "model2_val_X = np.array(model2_val_X)\n",
    "model2_val_X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset.X['features'].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = merged_model.fit([model2_train_X, train_dataset.X['points'], train_dataset.X['features'], train_dataset.X['mask']], train_dataset.y,\n",
    "          batch_size=batch_size,\n",
    "          epochs=3000,\n",
    "          validation_data=([model2_val_X, val_dataset.X['points'],  val_dataset.X['features'],  val_dataset.X['mask'],], val_dataset.y),\n",
    "          class_weight=class_weights,\n",
    "          shuffle=True,\n",
    "          verbose=2,                 \n",
    "          callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_model.save(OUTPUT) # save it to disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(history.history['loss'], label='train')\n",
    "plt.plot(history.history['val_loss'], label='val')\n",
    "plt.title('Loss function of the model')\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('loss')\n",
    "plt.legend()\n",
    "plt.savefig('{}/loss.pdf'.format(OUTPUT))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# accuracy\n",
    "fig = plt.figure()\n",
    "plt.plot(history.history['binary_accuracy'], label='train')\n",
    "plt.plot(history.history['val_binary_accuracy'], label='val')\n",
    "plt.title('Accuracy of the model')\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('binary accuracy')\n",
    "plt.legend()\n",
    "plt.hlines(xmin=1, xmax=len(history.history['binary_accuracy']), y=0.8, color='green', linestyle='--')\n",
    "plt.hlines(xmin=1, xmax=len(history.history['binary_accuracy']), y=0.9, color='green', linestyle=':')\n",
    "plt.vlines(ymin=np.min(history.history['val_binary_accuracy']), ymax=0.9, x = len(history.history['val_binary_accuracy'])-51.5, color='red',linestyle=':')\n",
    "plt.savefig('{}/accuracy.pdf'.format(OUTPUT))\n",
    "# plt.show()\n",
    "# plt.savefig('{}/score.pdf'.format(OUTPUT))\n",
    "# history.history['accuracy']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
