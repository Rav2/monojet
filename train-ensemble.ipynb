{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import awkward0 as awkward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 0 # random seed corresponds to the member of the ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "\n",
    "# uncomment and adjust for GPU calculations\n",
    "# os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\"\n",
    "# # dynamic memory growth\n",
    "# physical_devices = tf.config.list_physical_devices('GPU')\n",
    "# tf.config.experimental.set_memory_growth(physical_devices[0], True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome to JupyROOT 6.24/02\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "from os.path import join\n",
    "from ROOT import TLorentzVector\n",
    "from tensorflow.keras.optimizers import *\n",
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras import regularizers\n",
    "import seaborn as sns\n",
    "from tensorflow.keras.utils import *\n",
    "import shutil\n",
    "from dataset import *\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(SEED)\n",
    "np.random.seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "NO_OF_PARTICLES = 250 # how many particles in an event to use, default 250\n",
    "PT_CUT = 1.0 # Lower cut on PT particles, default is 1.0\n",
    "OUTPUT = f'model-{SEED}' # name of the output folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(OUTPUT):\n",
    "  # Create a new directory because it does not exist \n",
    "  os.makedirs(OUTPUT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "logging.basicConfig(filename=join(OUTPUT, 'training.log'),\n",
    "                    filemode='a',\n",
    "                    format='%(asctime)s,%(msecs)d %(name)s %(levelname)s %(message)s',\n",
    "                    datefmt='%H:%M:%S',\n",
    "                    level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "IPython.notebook.save_notebook()\n"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%javascript\n",
    "IPython.notebook.save_notebook()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'attempt143-8/keras_train5-ENSEMBLE.ipynb'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shutil.copy('train-ensemble.ipynb', join(OUTPUT, 'train-ensemble.ipynb')) # copy notebook to the output dir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load validation set. Adjust the data folder if needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keys in the datafile found:\n",
      "['track_momentum', 'track_pt', 'track_phi', 'track_eta', 'track_charge', 'track_origin', 'photon_energy', 'photon_ET', 'photon_phi', 'photon_eta', 'photon_charge', 'photon_origin', 'hadron_energy', 'hadron_ET', 'hadron_phi', 'hadron_eta', 'hadron_charge', 'hadron_origin', 'event_pt', 'event_eta', 'event_phi', 'event_mass', 'label', 'train_val_test', 'n_parts', 'jet_PT', 'jet_Eta', 'jet_Phi', 'jet_Mass', 'jet_Flavor', 'con_jet1_phi_arr', 'con_jet2_phi_arr', 'con_jet3_phi_arr', 'con_jet1_eta_arr', 'con_jet2_eta_arr', 'con_jet3_eta_arr', 'con_jet1_pt_arr', 'con_jet2_pt_arr', 'con_jet3_pt_arr', 'con_jet1_type_arr', 'con_jet2_type_arr', 'con_jet3_type_arr', 'origin', 'part_pt_log', 'part_ptrel', 'part_logptrel', 'part_e_log', 'part_erel', 'part_logerel', 'squark_pt', 'part_raw_etarel', 'part_etarel', 'part_phirel', 'part_deltaR', 'event_MET', 'event_HT', 'event_MT2']\n",
      "Loading ['part_etarel', 'part_phirel']\n",
      "Loading ['part_etarel', 'part_phirel']\n",
      "Loading ['part_pt_log', 'part_e_log', 'part_etarel', 'part_phirel', 'origin']\n",
      "Loading ['part_pt_log', 'part_e_log', 'part_etarel', 'part_phirel', 'origin']\n",
      "Loading ['part_pt_log', 'part_e_log', 'part_etarel', 'part_phirel', 'origin']\n",
      "Loading ['part_pt_log', 'part_e_log', 'part_etarel', 'part_phirel', 'origin']\n",
      "Loading ['part_pt_log', 'part_e_log', 'part_etarel', 'part_phirel', 'origin']\n",
      "Loading ['part_pt_log']\n",
      "Loading ['origin']\n",
      "Loading ['track_pt', 'track_eta', 'track_phi']\n",
      "Loading ['track_pt', 'track_eta', 'track_phi']\n",
      "Loading ['track_pt', 'track_eta', 'track_phi']\n",
      "Loading ['photon_ET', 'photon_eta', 'photon_phi']\n",
      "Loading ['photon_ET', 'photon_eta', 'photon_phi']\n",
      "Loading ['photon_ET', 'photon_eta', 'photon_phi']\n",
      "Loading ['hadron_ET', 'hadron_eta', 'hadron_phi']\n",
      "Loading ['hadron_ET', 'hadron_eta', 'hadron_phi']\n",
      "Loading ['hadron_ET', 'hadron_eta', 'hadron_phi']\n",
      "Loading ['jet_PT', 'jet_Eta', 'jet_Phi', 'jet_Mass', 'jet_Flavor']\n",
      "Loading ['jet_PT', 'jet_Eta', 'jet_Phi', 'jet_Mass', 'jet_Flavor']\n",
      "Loading ['jet_PT', 'jet_Eta', 'jet_Phi', 'jet_Mass', 'jet_Flavor']\n",
      "Loading ['jet_PT', 'jet_Eta', 'jet_Phi', 'jet_Mass', 'jet_Flavor']\n",
      "Loading ['jet_PT', 'jet_Eta', 'jet_Phi', 'jet_Mass', 'jet_Flavor']\n",
      "Loading ['squark_pt']\n",
      "Loading ['con_jet1_phi_arr', 'con_jet1_eta_arr', 'con_jet1_pt_arr', 'con_jet1_type_arr']\n",
      "Loading ['con_jet1_phi_arr', 'con_jet1_eta_arr', 'con_jet1_pt_arr', 'con_jet1_type_arr']\n",
      "Loading ['con_jet1_phi_arr', 'con_jet1_eta_arr', 'con_jet1_pt_arr', 'con_jet1_type_arr']\n",
      "Loading ['con_jet1_phi_arr', 'con_jet1_eta_arr', 'con_jet1_pt_arr', 'con_jet1_type_arr']\n",
      "Loading ['con_jet2_phi_arr', 'con_jet2_eta_arr', 'con_jet2_pt_arr', 'con_jet2_type_arr']\n",
      "Loading ['con_jet2_phi_arr', 'con_jet2_eta_arr', 'con_jet2_pt_arr', 'con_jet2_type_arr']\n",
      "Loading ['con_jet2_phi_arr', 'con_jet2_eta_arr', 'con_jet2_pt_arr', 'con_jet2_type_arr']\n",
      "Loading ['con_jet2_phi_arr', 'con_jet2_eta_arr', 'con_jet2_pt_arr', 'con_jet2_type_arr']\n",
      "Loading ['con_jet3_phi_arr', 'con_jet3_eta_arr', 'con_jet3_pt_arr', 'con_jet3_type_arr']\n",
      "Loading ['con_jet3_phi_arr', 'con_jet3_eta_arr', 'con_jet3_pt_arr', 'con_jet3_type_arr']\n",
      "Loading ['con_jet3_phi_arr', 'con_jet3_eta_arr', 'con_jet3_pt_arr', 'con_jet3_type_arr']\n",
      "Loading ['con_jet3_phi_arr', 'con_jet3_eta_arr', 'con_jet3_pt_arr', 'con_jet3_type_arr']\n",
      "Dataset created!\n"
     ]
    }
   ],
   "source": [
    "val_dataset = Dataset(f'/data/higgsino_train/ensemble_val_{SEED}.awkd', {}, data_format='channel_last', simple_mode=False, pad_len=NO_OF_PARTICLES)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load training set. Adjust the data folder if needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keys in the datafile found:\n",
      "['track_momentum', 'track_pt', 'track_phi', 'track_eta', 'track_charge', 'track_origin', 'photon_energy', 'photon_ET', 'photon_phi', 'photon_eta', 'photon_charge', 'photon_origin', 'hadron_energy', 'hadron_ET', 'hadron_phi', 'hadron_eta', 'hadron_charge', 'hadron_origin', 'event_pt', 'event_eta', 'event_phi', 'event_mass', 'label', 'train_val_test', 'n_parts', 'jet_PT', 'jet_Eta', 'jet_Phi', 'jet_Mass', 'jet_Flavor', 'con_jet1_phi_arr', 'con_jet2_phi_arr', 'con_jet3_phi_arr', 'con_jet1_eta_arr', 'con_jet2_eta_arr', 'con_jet3_eta_arr', 'con_jet1_pt_arr', 'con_jet2_pt_arr', 'con_jet3_pt_arr', 'con_jet1_type_arr', 'con_jet2_type_arr', 'con_jet3_type_arr', 'origin', 'part_pt_log', 'part_ptrel', 'part_logptrel', 'part_e_log', 'part_erel', 'part_logerel', 'squark_pt', 'part_raw_etarel', 'part_etarel', 'part_phirel', 'part_deltaR', 'event_MET', 'event_HT', 'event_MT2']\n",
      "Loading ['part_etarel', 'part_phirel']\n",
      "Loading ['part_etarel', 'part_phirel']\n",
      "Loading ['part_pt_log', 'part_e_log', 'part_etarel', 'part_phirel', 'origin']\n",
      "Loading ['part_pt_log', 'part_e_log', 'part_etarel', 'part_phirel', 'origin']\n",
      "Loading ['part_pt_log', 'part_e_log', 'part_etarel', 'part_phirel', 'origin']\n",
      "Loading ['part_pt_log', 'part_e_log', 'part_etarel', 'part_phirel', 'origin']\n",
      "Loading ['part_pt_log', 'part_e_log', 'part_etarel', 'part_phirel', 'origin']\n",
      "Loading ['part_pt_log']\n",
      "Loading ['origin']\n",
      "Loading ['track_pt', 'track_eta', 'track_phi']\n",
      "Loading ['track_pt', 'track_eta', 'track_phi']\n",
      "Loading ['track_pt', 'track_eta', 'track_phi']\n",
      "Loading ['photon_ET', 'photon_eta', 'photon_phi']\n",
      "Loading ['photon_ET', 'photon_eta', 'photon_phi']\n",
      "Loading ['photon_ET', 'photon_eta', 'photon_phi']\n",
      "Loading ['hadron_ET', 'hadron_eta', 'hadron_phi']\n",
      "Loading ['hadron_ET', 'hadron_eta', 'hadron_phi']\n",
      "Loading ['hadron_ET', 'hadron_eta', 'hadron_phi']\n",
      "Loading ['jet_PT', 'jet_Eta', 'jet_Phi', 'jet_Mass', 'jet_Flavor']\n",
      "Loading ['jet_PT', 'jet_Eta', 'jet_Phi', 'jet_Mass', 'jet_Flavor']\n",
      "Loading ['jet_PT', 'jet_Eta', 'jet_Phi', 'jet_Mass', 'jet_Flavor']\n",
      "Loading ['jet_PT', 'jet_Eta', 'jet_Phi', 'jet_Mass', 'jet_Flavor']\n",
      "Loading ['jet_PT', 'jet_Eta', 'jet_Phi', 'jet_Mass', 'jet_Flavor']\n",
      "Loading ['squark_pt']\n",
      "Loading ['con_jet1_phi_arr', 'con_jet1_eta_arr', 'con_jet1_pt_arr', 'con_jet1_type_arr']\n",
      "Loading ['con_jet1_phi_arr', 'con_jet1_eta_arr', 'con_jet1_pt_arr', 'con_jet1_type_arr']\n",
      "Loading ['con_jet1_phi_arr', 'con_jet1_eta_arr', 'con_jet1_pt_arr', 'con_jet1_type_arr']\n",
      "Loading ['con_jet1_phi_arr', 'con_jet1_eta_arr', 'con_jet1_pt_arr', 'con_jet1_type_arr']\n",
      "Loading ['con_jet2_phi_arr', 'con_jet2_eta_arr', 'con_jet2_pt_arr', 'con_jet2_type_arr']\n",
      "Loading ['con_jet2_phi_arr', 'con_jet2_eta_arr', 'con_jet2_pt_arr', 'con_jet2_type_arr']\n",
      "Loading ['con_jet2_phi_arr', 'con_jet2_eta_arr', 'con_jet2_pt_arr', 'con_jet2_type_arr']\n",
      "Loading ['con_jet2_phi_arr', 'con_jet2_eta_arr', 'con_jet2_pt_arr', 'con_jet2_type_arr']\n",
      "Loading ['con_jet3_phi_arr', 'con_jet3_eta_arr', 'con_jet3_pt_arr', 'con_jet3_type_arr']\n",
      "Loading ['con_jet3_phi_arr', 'con_jet3_eta_arr', 'con_jet3_pt_arr', 'con_jet3_type_arr']\n",
      "Loading ['con_jet3_phi_arr', 'con_jet3_eta_arr', 'con_jet3_pt_arr', 'con_jet3_type_arr']\n",
      "Loading ['con_jet3_phi_arr', 'con_jet3_eta_arr', 'con_jet3_pt_arr', 'con_jet3_type_arr']\n",
      "Dataset created!\n"
     ]
    }
   ],
   "source": [
    "train_dataset = Dataset(f'/eflow_data/higgsino_train/ensemble_train_{SEED}.awkd', {}, data_format='channel_last', simple_mode=False, pad_len=NO_OF_PARTICLES)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate means and standard deviations, and normalise the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "scalers = train_dataset.normalize_all(scalers = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'scaler_phi': StandardScaler(),\n",
       " 'scaler_eta': StandardScaler(),\n",
       " 'scaler_met': StandardScaler(),\n",
       " 'scaler_MT2': StandardScaler(),\n",
       " 'scaler_ht': StandardScaler(),\n",
       " 'scaler_m': StandardScaler(),\n",
       " 'scaler_jet1PT': StandardScaler(),\n",
       " 'scaler_jet1ETA': StandardScaler(),\n",
       " 'scaler_jet1MASS': StandardScaler(),\n",
       " 'scaler_jet1DPHI': StandardScaler(),\n",
       " 'scaler_jet2PT': StandardScaler(),\n",
       " 'scaler_jet2ETA': StandardScaler(),\n",
       " 'scaler_jet2MASS': StandardScaler(),\n",
       " 'scaler_jet2DPHI': StandardScaler(),\n",
       " 'scaler_jet3PT': StandardScaler(),\n",
       " 'scaler_jet3ETA': StandardScaler(),\n",
       " 'scaler_jet3MASS': StandardScaler(),\n",
       " 'scaler_jet3DPHI': StandardScaler(),\n",
       " 'scaler_jet4PT': StandardScaler(),\n",
       " 'scaler_jet4ETA': StandardScaler(),\n",
       " 'scaler_jet4MASS': StandardScaler(),\n",
       " 'scaler_jet4DPHI': StandardScaler()}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_dataset.normalize_all(scalers=scalers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scaler_phi\n",
      "mean: -0.0009872796078240246\n",
      "var: 3.2911722820461997\n",
      "scale: 1.814158835947448\n",
      "scaler_eta\n",
      "mean: 0.0009836877373430276\n",
      "var: 0.6764734905577952\n",
      "scale: 0.8224800852043745\n",
      "scaler_met\n",
      "mean: 1130.113519509007\n",
      "var: 84220.0697576037\n",
      "scale: 290.20694298655866\n",
      "scaler_MT2\n",
      "mean: 1007.596262032176\n",
      "var: 82579.14149917677\n",
      "scale: 287.36586696957727\n",
      "scaler_ht\n",
      "mean: 1546.1682843649069\n",
      "var: 231655.84256466498\n",
      "scale: 481.30639156847377\n",
      "scaler_m\n",
      "mean: 3052.7427181564426\n",
      "var: 977431.4137292544\n",
      "scale: 988.6513104878051\n",
      "scaler_jet1PT\n",
      "mean: 908.6745824742735\n",
      "var: 107958.66982526249\n",
      "scale: 328.5706466275746\n",
      "scaler_jet1ETA\n",
      "mean: 0.0005679062923058418\n",
      "var: 0.5543616500942882\n",
      "scale: 0.7445546656185079\n",
      "scaler_jet1MASS\n",
      "mean: 77.5006601535009\n",
      "var: 2698.2103036196477\n",
      "scale: 51.94430001087365\n",
      "scaler_jet1DPHI\n",
      "mean: -0.000466294407694618\n",
      "var: 0.2287676683491307\n",
      "scale: 0.4782966321741464\n",
      "scaler_jet2PT\n",
      "mean: 527.7618575955822\n",
      "var: 43028.34287538092\n",
      "scale: 207.43274301657615\n",
      "scaler_jet2ETA\n",
      "mean: 0.0011419683274109748\n",
      "var: 0.7812077319791684\n",
      "scale: 0.8838595657564432\n",
      "scaler_jet2MASS\n",
      "mean: 50.17552671242663\n",
      "var: 986.8083555662264\n",
      "scale: 31.413505941970666\n",
      "scaler_jet2DPHI\n",
      "mean: -0.0007439225641632854\n",
      "var: 1.0456884108776343\n",
      "scale: 1.0225890723441329\n",
      "scaler_jet3PT\n",
      "mean: 80.87879566105282\n",
      "var: 11564.93247141046\n",
      "scale: 107.54037600552854\n",
      "scaler_jet3ETA\n",
      "mean: -0.0011173855536948192\n",
      "var: 1.3221051352541973\n",
      "scale: 1.1498283068589836\n",
      "scaler_jet3MASS\n",
      "mean: 11.040639990881532\n",
      "var: 196.27051797119674\n",
      "scale: 14.009658024776934\n",
      "scaler_jet3DPHI\n",
      "mean: 0.0012491657727846585\n",
      "var: 0.8979116465309871\n",
      "scale: 0.9475819998981551\n",
      "scaler_jet4PT\n",
      "mean: 15.785469121036874\n",
      "var: 1190.2937992992588\n",
      "scale: 34.50063476661348\n",
      "scaler_jet4ETA\n",
      "mean: 0.0014120086883982852\n",
      "var: 0.7072035598281338\n",
      "scale: 0.8409539582094455\n",
      "scaler_jet4MASS\n",
      "mean: 2.629472241191608\n",
      "var: 30.01742357809799\n",
      "scale: 5.47881589196954\n",
      "scaler_jet4DPHI\n",
      "mean: 0.0003889664978931251\n",
      "var: 0.5117055443473356\n",
      "scale: 0.7153359660658309\n",
      "\n"
     ]
    }
   ],
   "source": [
    "string = ''\n",
    "for k,v in scalers.items():\n",
    "    string += k + '\\n'\n",
    "    string += f'mean: {v.mean_[0]}'+ '\\n'\n",
    "    string += f'var: {v.var_[0]}' + '\\n'\n",
    "    string += f'scale: {v.scale_[0]}' + '\\n'\n",
    "print(string)\n",
    "with open(join(OUTPUT, 'scaler.txt'), 'w') as f:\n",
    "    f.write(string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apply PT cut"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_pt_cut(data, cut=PT_CUT):\n",
    "    cond = data.X['mask'] < np.log(cut)\n",
    "    cond2 = data.X['mask'] >= np.log(cut)\n",
    "    print(cond.shape)\n",
    "    print('below the cut: ', np.sum(cond))\n",
    "    print('above the cut: ', np.sum(cond2))\n",
    "    print('sum: ', np.sum(cond) + np.sum(cond2))\n",
    "    data.X['mask'][cond] = 0.0\n",
    "    ext_shape = list(cond.shape)\n",
    "    ext_shape[-1] = 4\n",
    "    new_cond = np.repeat(cond, 5, axis=2)\n",
    "    print(new_cond.shape)\n",
    "    data.X['features'][new_cond] = 0.0\n",
    "  \n",
    "    return data, np.sum(cond2, axis=1).flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(817314, 250, 1)\n",
      "below the cut:  181989564\n",
      "above the cut:  22338936\n",
      "sum:  204328500\n",
      "(817314, 250, 5)\n",
      "(350277, 250, 1)\n",
      "below the cut:  78000615\n",
      "above the cut:  9568635\n",
      "sum:  87569250\n",
      "(350277, 250, 5)\n"
     ]
    }
   ],
   "source": [
    "train_dataset, _ = apply_pt_cut(train_dataset, PT_CUT)\n",
    "val_dataset, _ = apply_pt_cut(val_dataset, PT_CUT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the ParticleNet model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tf_keras_model import get_particle_net, get_particle_net_lite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'points': (250, 2), 'features': (250, 5), 'mask': (250, 1), 'origin': (250, 1), 'track': (250, 3), 'photon': (250, 3), 'hadron': (250, 3), 'jet': (4, 5), 'squark': (3, 1), 'jet1_con': (80, 4), 'jet2_con': (80, 4), 'jet3_con': (80, 4)}\n"
     ]
    }
   ],
   "source": [
    "model_type = 'particle_net_lite' # choose between 'particle_net' and 'particle_net_lite'\n",
    "num_classes = train_dataset.y.shape[1]\n",
    "input_shapes = {k:train_dataset[k].shape[1:] for k in train_dataset.X}\n",
    "print(input_shapes)\n",
    "if 'lite' in model_type:\n",
    "    model = get_particle_net_lite(num_classes, input_shapes)\n",
    "else:\n",
    "    model = get_particle_net(num_classes, input_shapes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create a dense network for high-level variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2_inputs = keras.Input(shape=(5+4*4,))\n",
    "xx = model2_inputs\n",
    "xx = keras.layers.Dense(256, activation=None, kernel_regularizer=keras.regularizers.L2(1e-3))(xx)\n",
    "xx = keras.layers.BatchNormalization()(xx)\n",
    "xx = keras.layers.Activation(tf.nn.relu)(xx)\n",
    "for ii in range(5):\n",
    "    xx = keras.layers.Dense(128, activation=None, kernel_regularizer=keras.regularizers.L2(1e-3))(xx)\n",
    "    xx = keras.layers.BatchNormalization()(xx)\n",
    "    xx = keras.layers.Activation(tf.nn.relu)(xx)    \n",
    "xx = keras.layers.Dropout(0.5)(xx)\n",
    "xx = keras.layers.Dense(2, activation=\"softmax\",)(xx)\n",
    "\n",
    "model2 = keras.Model(model2_inputs, xx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2.compile(loss='binary_crossentropy',\n",
    "              optimizer=keras.optimizers.Adam(learning_rate=5e-4), #lr_scheduler),\n",
    "              metrics = [tf.keras.metrics.BinaryAccuracy()],\n",
    "              weighted_metrics=[tf.keras.metrics.AUC()])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modify the GNN part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Improved particle net\n",
    "yy = model.layers[-4].output\n",
    "yy = keras.layers.Dense(256, activation=None, kernel_regularizer=keras.regularizers.L2(1e-3))(yy)\n",
    "yy = keras.layers.BatchNormalization()(yy)\n",
    "yy = keras.layers.Activation(tf.nn.relu)(yy)\n",
    "yy = keras.layers.Dropout(0.5)(yy)\n",
    "yy = keras.layers.Dense(128, activation=None, kernel_regularizer=keras.regularizers.L2(1e-3))(yy)\n",
    "yy = keras.layers.BatchNormalization()(yy)\n",
    "yy = keras.layers.Activation(tf.nn.relu)(yy)\n",
    "\n",
    "model3 = keras.Model(model.input, yy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Merge two models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_layer = keras.layers.Concatenate()([model2.layers[-3].output, model3.layers[-1].output])\n",
    "z = merged_layer\n",
    "z = keras.layers.Dense(128, activation=None, kernel_regularizer=keras.regularizers.L2(1e-3))(z)\n",
    "z = keras.layers.BatchNormalization()(z)\n",
    "z = keras.layers.Activation(tf.nn.relu)(z)\n",
    "z = keras.layers.Dropout(0.5)(z)\n",
    "z = keras.layers.Dense(2, activation=\"softmax\",)(z)\n",
    "merged_model = keras.Model([model2.input, model3.input], z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set the batch size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1024"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Warmup cosine decay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# based on https://stackabuse.com/learning-rate-warmup-with-cosine-decay-in-keras-and-tensorflow/\n",
    "from tensorflow.keras import backend as K\n",
    "\n",
    "def lr_warmup_cosine_decay(global_step,\n",
    "                           warmup_steps,\n",
    "                           hold = 0,\n",
    "                           total_steps=500,\n",
    "                           start_lr=0.0,\n",
    "                           target_lr=1e-3,\n",
    "                           final_lr=1e-5):\n",
    "    # Cosine decay\n",
    "    # There is no tf.pi so we wrap np.pi as a TF constant\n",
    "    learning_rate = final_lr+ 0.5 * target_lr * (1.0 + tf.cos(tf.constant(np.pi) * tf.convert_to_tensor(global_step - warmup_steps - hold, dtype=np.float32) / tf.convert_to_tensor(total_steps - warmup_steps - hold, dtype=np.float32)))\n",
    "\n",
    "    # Target LR * progress of warmup (=1 at the final warmup step)\n",
    "    warmup_lr = target_lr * (global_step / warmup_steps)\n",
    "\n",
    "    # Choose between `warmup_lr`, `target_lr` and `learning_rate` based on whether `global_step < warmup_steps` and we're still holding.\n",
    "    # i.e. warm up if we're still warming up and use cosine decayed lr otherwise\n",
    "    if hold > 0:\n",
    "        learning_rate = tf.where(global_step > warmup_steps + hold,\n",
    "                                 learning_rate, target_lr)\n",
    "    \n",
    "    learning_rate = tf.where(global_step < warmup_steps, warmup_lr, learning_rate)\n",
    "\n",
    "    return learning_rate\n",
    "\n",
    "\n",
    "\n",
    "class WarmUpCosineDecay(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "    def __init__(self, start_lr, target_lr, warmup_steps, total_steps, hold, final_lr):\n",
    "        super().__init__()\n",
    "        self.start_lr = start_lr\n",
    "        self.target_lr = target_lr\n",
    "        self.warmup_steps = warmup_steps\n",
    "        self.total_steps = total_steps\n",
    "        self.hold = hold\n",
    "        self.final_lr = final_lr\n",
    "    \n",
    "    def get_config(self):\n",
    "        config = {\n",
    "          'start_lr': self.start_lr,\n",
    "          'target_lr': self.target_lr,\n",
    "          'warmup_steps': self.warmup_steps,\n",
    "          'total_steps': self.total_steps,\n",
    "          'hold': self.hold,\n",
    "          'final_lr':self.final_lr,\n",
    "        }\n",
    "        return config\n",
    "\n",
    "    def __call__(self, step):\n",
    "        lr = lr_warmup_cosine_decay(global_step=tf.cast(step, np.float32),\n",
    "                                    total_steps=self.total_steps,\n",
    "                                    warmup_steps=self.warmup_steps,\n",
    "                                    start_lr=self.start_lr,\n",
    "                                    target_lr=self.target_lr,\n",
    "                                    hold=self.hold,\n",
    "                                    final_lr=self.final_lr)\n",
    "\n",
    "        return tf.where(\n",
    "            step > self.total_steps, self.final_lr, lr, name=\"learning_rate\"\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_schedule = WarmUpCosineDecay(start_lr=0, target_lr=5e-4, warmup_steps=10*800, total_steps=310*800, hold=0, final_lr=1e-5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " mask (InputLayer)              [(None, 250, 1)]     0           []                               \n",
      "                                                                                                  \n",
      " tf.math.not_equal (TFOpLambda)  (None, 250, 1)      0           ['mask[0][0]']                   \n",
      "                                                                                                  \n",
      " tf.cast (TFOpLambda)           (None, 250, 1)       0           ['tf.math.not_equal[0][0]']      \n",
      "                                                                                                  \n",
      " tf.math.equal (TFOpLambda)     (None, 250, 1)       0           ['tf.cast[0][0]']                \n",
      "                                                                                                  \n",
      " tf.cast_1 (TFOpLambda)         (None, 250, 1)       0           ['tf.math.equal[0][0]']          \n",
      "                                                                                                  \n",
      " tf.math.multiply (TFOpLambda)  (None, 250, 1)       0           ['tf.cast_1[0][0]']              \n",
      "                                                                                                  \n",
      " points (InputLayer)            [(None, 250, 2)]     0           []                               \n",
      "                                                                                                  \n",
      " tf.math.add (TFOpLambda)       (None, 250, 2)       0           ['tf.math.multiply[0][0]',       \n",
      "                                                                  'points[0][0]']                 \n",
      "                                                                                                  \n",
      " features (InputLayer)          [(None, 250, 5)]     0           []                               \n",
      "                                                                                                  \n",
      " tf.compat.v1.transpose (TFOpLa  (None, 2, 250)      0           ['tf.math.add[0][0]']            \n",
      " mbda)                                                                                            \n",
      "                                                                                                  \n",
      " tf.expand_dims (TFOpLambda)    (None, 250, 1, 5)    0           ['features[0][0]']               \n",
      "                                                                                                  \n",
      " tf.math.multiply_1 (TFOpLambda  (None, 250, 2)      0           ['tf.math.add[0][0]',            \n",
      " )                                                                'tf.math.add[0][0]']            \n",
      "                                                                                                  \n",
      " tf.linalg.matmul (TFOpLambda)  (None, 250, 250)     0           ['tf.math.add[0][0]',            \n",
      "                                                                  'tf.compat.v1.transpose[0][0]'] \n",
      "                                                                                                  \n",
      " tf.math.multiply_2 (TFOpLambda  (None, 250, 2)      0           ['tf.math.add[0][0]',            \n",
      " )                                                                'tf.math.add[0][0]']            \n",
      "                                                                                                  \n",
      " ParticleNet_fts_bn (BatchNorma  (None, 250, 1, 5)   20          ['tf.expand_dims[0][0]']         \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " tf.math.reduce_sum (TFOpLambda  (None, 250, 1)      0           ['tf.math.multiply_1[0][0]']     \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " tf.math.multiply_3 (TFOpLambda  (None, 250, 250)    0           ['tf.linalg.matmul[0][0]']       \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " tf.math.reduce_sum_1 (TFOpLamb  (None, 250, 1)      0           ['tf.math.multiply_2[0][0]']     \n",
      " da)                                                                                              \n",
      "                                                                                                  \n",
      " tf.compat.v1.squeeze (TFOpLamb  (None, 250, 5)      0           ['ParticleNet_fts_bn[0][0]']     \n",
      " da)                                                                                              \n",
      "                                                                                                  \n",
      " tf.math.subtract (TFOpLambda)  (None, 250, 250)     0           ['tf.math.reduce_sum[0][0]',     \n",
      "                                                                  'tf.math.multiply_3[0][0]']     \n",
      "                                                                                                  \n",
      " tf.compat.v1.transpose_1 (TFOp  (None, 1, 250)      0           ['tf.math.reduce_sum_1[0][0]']   \n",
      " Lambda)                                                                                          \n",
      "                                                                                                  \n",
      " tf.compat.v1.shape (TFOpLambda  (3,)                0           ['tf.compat.v1.squeeze[0][0]']   \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " tf.__operators__.add (TFOpLamb  (None, 250, 250)    0           ['tf.math.subtract[0][0]',       \n",
      " da)                                                              'tf.compat.v1.transpose_1[0][0]'\n",
      "                                                                 ]                                \n",
      "                                                                                                  \n",
      " tf.__operators__.getitem_1 (Sl  ()                  0           ['tf.compat.v1.shape[0][0]']     \n",
      " icingOpLambda)                                                                                   \n",
      "                                                                                                  \n",
      " tf.math.negative (TFOpLambda)  (None, 250, 250)     0           ['tf.__operators__.add[0][0]']   \n",
      "                                                                                                  \n",
      " tf.range (TFOpLambda)          (None,)              0           ['tf.__operators__.getitem_1[0][0\n",
      "                                                                 ]']                              \n",
      "                                                                                                  \n",
      " tf.math.top_k (TFOpLambda)     TopKV2(values=(None  0           ['tf.math.negative[0][0]']       \n",
      "                                , 250, 8),                                                        \n",
      "                                 indices=(None, 250                                               \n",
      "                                , 8))                                                             \n",
      "                                                                                                  \n",
      " tf.reshape (TFOpLambda)        (None, 1, 1, 1)      0           ['tf.range[0][0]']               \n",
      "                                                                                                  \n",
      " tf.__operators__.getitem (Slic  (None, 250, 7)      0           ['tf.math.top_k[0][1]']          \n",
      " ingOpLambda)                                                                                     \n",
      "                                                                                                  \n",
      " tf.tile (TFOpLambda)           (None, 250, 7, 1)    0           ['tf.reshape[0][0]']             \n",
      "                                                                                                  \n",
      " tf.expand_dims_1 (TFOpLambda)  (None, 250, 7, 1)    0           ['tf.__operators__.getitem[0][0]'\n",
      "                                                                 ]                                \n",
      "                                                                                                  \n",
      " tf.expand_dims_2 (TFOpLambda)  (None, 250, 1, 5)    0           ['tf.compat.v1.squeeze[0][0]']   \n",
      "                                                                                                  \n",
      " tf.concat (TFOpLambda)         (None, 250, 7, 2)    0           ['tf.tile[0][0]',                \n",
      "                                                                  'tf.expand_dims_1[0][0]']       \n",
      "                                                                                                  \n",
      " tf.tile_1 (TFOpLambda)         (None, 250, 7, 5)    0           ['tf.expand_dims_2[0][0]']       \n",
      "                                                                                                  \n",
      " tf.compat.v1.gather_nd (TFOpLa  (None, 250, 7, 5)   0           ['tf.compat.v1.squeeze[0][0]',   \n",
      " mbda)                                                            'tf.concat[0][0]']              \n",
      "                                                                                                  \n",
      " tf.math.subtract_1 (TFOpLambda  (None, 250, 7, 5)   0           ['tf.compat.v1.gather_nd[0][0]', \n",
      " )                                                                'tf.tile_1[0][0]']              \n",
      "                                                                                                  \n",
      " tf.concat_1 (TFOpLambda)       (None, 250, 7, 10)   0           ['tf.tile_1[0][0]',              \n",
      "                                                                  'tf.math.subtract_1[0][0]']     \n",
      "                                                                                                  \n",
      " ParticleNet_EdgeConv0_conv0 (C  (None, 250, 7, 32)  320         ['tf.concat_1[0][0]']            \n",
      " onv2D)                                                                                           \n",
      "                                                                                                  \n",
      " ParticleNet_EdgeConv0_bn0 (Bat  (None, 250, 7, 32)  128         ['ParticleNet_EdgeConv0_conv0[0][\n",
      " chNormalization)                                                0]']                             \n",
      "                                                                                                  \n",
      " ParticleNet_EdgeConv0_act0 (Ac  (None, 250, 7, 32)  0           ['ParticleNet_EdgeConv0_bn0[0][0]\n",
      " tivation)                                                       ']                               \n",
      "                                                                                                  \n",
      " ParticleNet_EdgeConv0_conv1 (C  (None, 250, 7, 32)  1024        ['ParticleNet_EdgeConv0_act0[0][0\n",
      " onv2D)                                                          ]']                              \n",
      "                                                                                                  \n",
      " ParticleNet_EdgeConv0_bn1 (Bat  (None, 250, 7, 32)  128         ['ParticleNet_EdgeConv0_conv1[0][\n",
      " chNormalization)                                                0]']                             \n",
      "                                                                                                  \n",
      " ParticleNet_EdgeConv0_act1 (Ac  (None, 250, 7, 32)  0           ['ParticleNet_EdgeConv0_bn1[0][0]\n",
      " tivation)                                                       ']                               \n",
      "                                                                                                  \n",
      " tf.expand_dims_3 (TFOpLambda)  (None, 250, 1, 5)    0           ['tf.compat.v1.squeeze[0][0]']   \n",
      "                                                                                                  \n",
      " ParticleNet_EdgeConv0_conv2 (C  (None, 250, 7, 32)  1024        ['ParticleNet_EdgeConv0_act1[0][0\n",
      " onv2D)                                                          ]']                              \n",
      "                                                                                                  \n",
      " ParticleNet_EdgeConv0_sc_conv   (None, 250, 1, 32)  160         ['tf.expand_dims_3[0][0]']       \n",
      " (Conv2D)                                                                                         \n",
      "                                                                                                  \n",
      " ParticleNet_EdgeConv0_bn2 (Bat  (None, 250, 7, 32)  128         ['ParticleNet_EdgeConv0_conv2[0][\n",
      " chNormalization)                                                0]']                             \n",
      "                                                                                                  \n",
      " ParticleNet_EdgeConv0_sc_bn (B  (None, 250, 1, 32)  128         ['ParticleNet_EdgeConv0_sc_conv[0\n",
      " atchNormalization)                                              ][0]']                           \n",
      "                                                                                                  \n",
      " ParticleNet_EdgeConv0_act2 (Ac  (None, 250, 7, 32)  0           ['ParticleNet_EdgeConv0_bn2[0][0]\n",
      " tivation)                                                       ']                               \n",
      "                                                                                                  \n",
      " tf.compat.v1.squeeze_1 (TFOpLa  (None, 250, 32)     0           ['ParticleNet_EdgeConv0_sc_bn[0][\n",
      " mbda)                                                           0]']                             \n",
      "                                                                                                  \n",
      " tf.math.reduce_mean (TFOpLambd  (None, 250, 32)     0           ['ParticleNet_EdgeConv0_act2[0][0\n",
      " a)                                                              ]']                              \n",
      "                                                                                                  \n",
      " tf.__operators__.add_1 (TFOpLa  (None, 250, 32)     0           ['tf.compat.v1.squeeze_1[0][0]', \n",
      " mbda)                                                            'tf.math.reduce_mean[0][0]']    \n",
      "                                                                                                  \n",
      " ParticleNet_EdgeConv0_sc_act (  (None, 250, 32)     0           ['tf.__operators__.add_1[0][0]'] \n",
      " Activation)                                                                                      \n",
      "                                                                                                  \n",
      " tf.math.add_1 (TFOpLambda)     (None, 250, 32)      0           ['tf.math.multiply[0][0]',       \n",
      "                                                                  'ParticleNet_EdgeConv0_sc_act[0]\n",
      "                                                                 [0]']                            \n",
      "                                                                                                  \n",
      " tf.compat.v1.transpose_2 (TFOp  (None, 32, 250)     0           ['tf.math.add_1[0][0]']          \n",
      " Lambda)                                                                                          \n",
      "                                                                                                  \n",
      " tf.math.multiply_4 (TFOpLambda  (None, 250, 32)     0           ['tf.math.add_1[0][0]',          \n",
      " )                                                                'tf.math.add_1[0][0]']          \n",
      "                                                                                                  \n",
      " tf.linalg.matmul_1 (TFOpLambda  (None, 250, 250)    0           ['tf.math.add_1[0][0]',          \n",
      " )                                                                'tf.compat.v1.transpose_2[0][0]'\n",
      "                                                                 ]                                \n",
      "                                                                                                  \n",
      " tf.math.multiply_5 (TFOpLambda  (None, 250, 32)     0           ['tf.math.add_1[0][0]',          \n",
      " )                                                                'tf.math.add_1[0][0]']          \n",
      "                                                                                                  \n",
      " tf.math.reduce_sum_2 (TFOpLamb  (None, 250, 1)      0           ['tf.math.multiply_4[0][0]']     \n",
      " da)                                                                                              \n",
      "                                                                                                  \n",
      " tf.math.multiply_6 (TFOpLambda  (None, 250, 250)    0           ['tf.linalg.matmul_1[0][0]']     \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " tf.math.reduce_sum_3 (TFOpLamb  (None, 250, 1)      0           ['tf.math.multiply_5[0][0]']     \n",
      " da)                                                                                              \n",
      "                                                                                                  \n",
      " tf.math.subtract_2 (TFOpLambda  (None, 250, 250)    0           ['tf.math.reduce_sum_2[0][0]',   \n",
      " )                                                                'tf.math.multiply_6[0][0]']     \n",
      "                                                                                                  \n",
      " tf.compat.v1.transpose_3 (TFOp  (None, 1, 250)      0           ['tf.math.reduce_sum_3[0][0]']   \n",
      " Lambda)                                                                                          \n",
      "                                                                                                  \n",
      " tf.compat.v1.shape_1 (TFOpLamb  (3,)                0           ['ParticleNet_EdgeConv0_sc_act[0]\n",
      " da)                                                             [0]']                            \n",
      "                                                                                                  \n",
      " tf.__operators__.add_2 (TFOpLa  (None, 250, 250)    0           ['tf.math.subtract_2[0][0]',     \n",
      " mbda)                                                            'tf.compat.v1.transpose_3[0][0]'\n",
      "                                                                 ]                                \n",
      "                                                                                                  \n",
      " tf.__operators__.getitem_3 (Sl  ()                  0           ['tf.compat.v1.shape_1[0][0]']   \n",
      " icingOpLambda)                                                                                   \n",
      "                                                                                                  \n",
      " tf.math.negative_1 (TFOpLambda  (None, 250, 250)    0           ['tf.__operators__.add_2[0][0]'] \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " tf.range_1 (TFOpLambda)        (None,)              0           ['tf.__operators__.getitem_3[0][0\n",
      "                                                                 ]']                              \n",
      "                                                                                                  \n",
      " tf.math.top_k_1 (TFOpLambda)   TopKV2(values=(None  0           ['tf.math.negative_1[0][0]']     \n",
      "                                , 250, 8),                                                        \n",
      "                                 indices=(None, 250                                               \n",
      "                                , 8))                                                             \n",
      "                                                                                                  \n",
      " tf.reshape_1 (TFOpLambda)      (None, 1, 1, 1)      0           ['tf.range_1[0][0]']             \n",
      "                                                                                                  \n",
      " tf.__operators__.getitem_2 (Sl  (None, 250, 7)      0           ['tf.math.top_k_1[0][1]']        \n",
      " icingOpLambda)                                                                                   \n",
      "                                                                                                  \n",
      " tf.tile_2 (TFOpLambda)         (None, 250, 7, 1)    0           ['tf.reshape_1[0][0]']           \n",
      "                                                                                                  \n",
      " tf.expand_dims_4 (TFOpLambda)  (None, 250, 7, 1)    0           ['tf.__operators__.getitem_2[0][0\n",
      "                                                                 ]']                              \n",
      "                                                                                                  \n",
      " tf.expand_dims_5 (TFOpLambda)  (None, 250, 1, 32)   0           ['ParticleNet_EdgeConv0_sc_act[0]\n",
      "                                                                 [0]']                            \n",
      "                                                                                                  \n",
      " tf.concat_2 (TFOpLambda)       (None, 250, 7, 2)    0           ['tf.tile_2[0][0]',              \n",
      "                                                                  'tf.expand_dims_4[0][0]']       \n",
      "                                                                                                  \n",
      " tf.tile_3 (TFOpLambda)         (None, 250, 7, 32)   0           ['tf.expand_dims_5[0][0]']       \n",
      "                                                                                                  \n",
      " tf.compat.v1.gather_nd_1 (TFOp  (None, 250, 7, 32)  0           ['ParticleNet_EdgeConv0_sc_act[0]\n",
      " Lambda)                                                         [0]',                            \n",
      "                                                                  'tf.concat_2[0][0]']            \n",
      "                                                                                                  \n",
      " tf.math.subtract_3 (TFOpLambda  (None, 250, 7, 32)  0           ['tf.compat.v1.gather_nd_1[0][0]'\n",
      " )                                                               , 'tf.tile_3[0][0]']             \n",
      "                                                                                                  \n",
      " tf.concat_3 (TFOpLambda)       (None, 250, 7, 64)   0           ['tf.tile_3[0][0]',              \n",
      "                                                                  'tf.math.subtract_3[0][0]']     \n",
      "                                                                                                  \n",
      " ParticleNet_EdgeConv1_conv0 (C  (None, 250, 7, 64)  4096        ['tf.concat_3[0][0]']            \n",
      " onv2D)                                                                                           \n",
      "                                                                                                  \n",
      " ParticleNet_EdgeConv1_bn0 (Bat  (None, 250, 7, 64)  256         ['ParticleNet_EdgeConv1_conv0[0][\n",
      " chNormalization)                                                0]']                             \n",
      "                                                                                                  \n",
      " input_1 (InputLayer)           [(None, 21)]         0           []                               \n",
      "                                                                                                  \n",
      " ParticleNet_EdgeConv1_act0 (Ac  (None, 250, 7, 64)  0           ['ParticleNet_EdgeConv1_bn0[0][0]\n",
      " tivation)                                                       ']                               \n",
      "                                                                                                  \n",
      " dense_2 (Dense)                (None, 256)          5632        ['input_1[0][0]']                \n",
      "                                                                                                  \n",
      " ParticleNet_EdgeConv1_conv1 (C  (None, 250, 7, 64)  4096        ['ParticleNet_EdgeConv1_act0[0][0\n",
      " onv2D)                                                          ]']                              \n",
      "                                                                                                  \n",
      " batch_normalization (BatchNorm  (None, 256)         1024        ['dense_2[0][0]']                \n",
      " alization)                                                                                       \n",
      "                                                                                                  \n",
      " ParticleNet_EdgeConv1_bn1 (Bat  (None, 250, 7, 64)  256         ['ParticleNet_EdgeConv1_conv1[0][\n",
      " chNormalization)                                                0]']                             \n",
      "                                                                                                  \n",
      " activation (Activation)        (None, 256)          0           ['batch_normalization[0][0]']    \n",
      "                                                                                                  \n",
      " ParticleNet_EdgeConv1_act1 (Ac  (None, 250, 7, 64)  0           ['ParticleNet_EdgeConv1_bn1[0][0]\n",
      " tivation)                                                       ']                               \n",
      "                                                                                                  \n",
      " dense_3 (Dense)                (None, 128)          32896       ['activation[0][0]']             \n",
      "                                                                                                  \n",
      " tf.expand_dims_6 (TFOpLambda)  (None, 250, 1, 32)   0           ['ParticleNet_EdgeConv0_sc_act[0]\n",
      "                                                                 [0]']                            \n",
      "                                                                                                  \n",
      " ParticleNet_EdgeConv1_conv2 (C  (None, 250, 7, 64)  4096        ['ParticleNet_EdgeConv1_act1[0][0\n",
      " onv2D)                                                          ]']                              \n",
      "                                                                                                  \n",
      " batch_normalization_1 (BatchNo  (None, 128)         512         ['dense_3[0][0]']                \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " ParticleNet_EdgeConv1_sc_conv   (None, 250, 1, 64)  2048        ['tf.expand_dims_6[0][0]']       \n",
      " (Conv2D)                                                                                         \n",
      "                                                                                                  \n",
      " ParticleNet_EdgeConv1_bn2 (Bat  (None, 250, 7, 64)  256         ['ParticleNet_EdgeConv1_conv2[0][\n",
      " chNormalization)                                                0]']                             \n",
      "                                                                                                  \n",
      " activation_1 (Activation)      (None, 128)          0           ['batch_normalization_1[0][0]']  \n",
      "                                                                                                  \n",
      " ParticleNet_EdgeConv1_sc_bn (B  (None, 250, 1, 64)  256         ['ParticleNet_EdgeConv1_sc_conv[0\n",
      " atchNormalization)                                              ][0]']                           \n",
      "                                                                                                  \n",
      " ParticleNet_EdgeConv1_act2 (Ac  (None, 250, 7, 64)  0           ['ParticleNet_EdgeConv1_bn2[0][0]\n",
      " tivation)                                                       ']                               \n",
      "                                                                                                  \n",
      " dense_4 (Dense)                (None, 128)          16512       ['activation_1[0][0]']           \n",
      "                                                                                                  \n",
      " tf.compat.v1.squeeze_2 (TFOpLa  (None, 250, 64)     0           ['ParticleNet_EdgeConv1_sc_bn[0][\n",
      " mbda)                                                           0]']                             \n",
      "                                                                                                  \n",
      " tf.math.reduce_mean_1 (TFOpLam  (None, 250, 64)     0           ['ParticleNet_EdgeConv1_act2[0][0\n",
      " bda)                                                            ]']                              \n",
      "                                                                                                  \n",
      " batch_normalization_2 (BatchNo  (None, 128)         512         ['dense_4[0][0]']                \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " tf.__operators__.add_3 (TFOpLa  (None, 250, 64)     0           ['tf.compat.v1.squeeze_2[0][0]', \n",
      " mbda)                                                            'tf.math.reduce_mean_1[0][0]']  \n",
      "                                                                                                  \n",
      " activation_2 (Activation)      (None, 128)          0           ['batch_normalization_2[0][0]']  \n",
      "                                                                                                  \n",
      " ParticleNet_EdgeConv1_sc_act (  (None, 250, 64)     0           ['tf.__operators__.add_3[0][0]'] \n",
      " Activation)                                                                                      \n",
      "                                                                                                  \n",
      " dense_5 (Dense)                (None, 128)          16512       ['activation_2[0][0]']           \n",
      "                                                                                                  \n",
      " tf.math.multiply_7 (TFOpLambda  (None, 250, 64)     0           ['ParticleNet_EdgeConv1_sc_act[0]\n",
      " )                                                               [0]',                            \n",
      "                                                                  'tf.cast[0][0]']                \n",
      "                                                                                                  \n",
      " batch_normalization_3 (BatchNo  (None, 128)         512         ['dense_5[0][0]']                \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " tf.math.reduce_mean_2 (TFOpLam  (None, 64)          0           ['tf.math.multiply_7[0][0]']     \n",
      " bda)                                                                                             \n",
      "                                                                                                  \n",
      " activation_3 (Activation)      (None, 128)          0           ['batch_normalization_3[0][0]']  \n",
      "                                                                                                  \n",
      " dense_9 (Dense)                (None, 256)          16640       ['tf.math.reduce_mean_2[0][0]']  \n",
      "                                                                                                  \n",
      " dense_6 (Dense)                (None, 128)          16512       ['activation_3[0][0]']           \n",
      "                                                                                                  \n",
      " batch_normalization_6 (BatchNo  (None, 256)         1024        ['dense_9[0][0]']                \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " batch_normalization_4 (BatchNo  (None, 128)         512         ['dense_6[0][0]']                \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " activation_6 (Activation)      (None, 256)          0           ['batch_normalization_6[0][0]']  \n",
      "                                                                                                  \n",
      " activation_4 (Activation)      (None, 128)          0           ['batch_normalization_4[0][0]']  \n",
      "                                                                                                  \n",
      " dropout_2 (Dropout)            (None, 256)          0           ['activation_6[0][0]']           \n",
      "                                                                                                  \n",
      " dense_7 (Dense)                (None, 128)          16512       ['activation_4[0][0]']           \n",
      "                                                                                                  \n",
      " dense_10 (Dense)               (None, 128)          32896       ['dropout_2[0][0]']              \n",
      "                                                                                                  \n",
      " batch_normalization_5 (BatchNo  (None, 128)         512         ['dense_7[0][0]']                \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " batch_normalization_7 (BatchNo  (None, 128)         512         ['dense_10[0][0]']               \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " activation_5 (Activation)      (None, 128)          0           ['batch_normalization_5[0][0]']  \n",
      "                                                                                                  \n",
      " activation_7 (Activation)      (None, 128)          0           ['batch_normalization_7[0][0]']  \n",
      "                                                                                                  \n",
      " concatenate (Concatenate)      (None, 256)          0           ['activation_5[0][0]',           \n",
      "                                                                  'activation_7[0][0]']           \n",
      "                                                                                                  \n",
      " dense_11 (Dense)               (None, 128)          32896       ['concatenate[0][0]']            \n",
      "                                                                                                  \n",
      " batch_normalization_8 (BatchNo  (None, 128)         512         ['dense_11[0][0]']               \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " activation_8 (Activation)      (None, 128)          0           ['batch_normalization_8[0][0]']  \n",
      "                                                                                                  \n",
      " dropout_3 (Dropout)            (None, 128)          0           ['activation_8[0][0]']           \n",
      "                                                                                                  \n",
      " dense_12 (Dense)               (None, 2)            258         ['dropout_3[0][0]']              \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 211,318\n",
      "Trainable params: 207,724\n",
      "Non-trainable params: 3,594\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "merged_model.compile(loss='binary_crossentropy',\n",
    "                     optimizer=keras.optimizers.Adam(learning_rate=lr_schedule, clipnorm=5e-4),\n",
    "                     metrics = [tf.keras.metrics.BinaryAccuracy()],\n",
    "                     weighted_metrics=[tf.keras.metrics.AUC()])\n",
    "merged_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare model checkpoint directory.\n",
    "import os\n",
    "save_dir = join(OUTPUT, 'model_checkpoints')\n",
    "model_name = 'model.{epoch:03d}.h5'\n",
    "if not os.path.isdir(save_dir):\n",
    "    os.makedirs(save_dir)\n",
    "filepath = os.path.join(save_dir, model_name)\n",
    "\n",
    "# Prepare callbacks for model saving and for learning rate adjustment.\n",
    "checkpoint = keras.callbacks.ModelCheckpoint(filepath=filepath,\n",
    "                             monitor='val_loss',\n",
    "                             verbose=1,\n",
    "                             save_best_only=True)\n",
    "\n",
    "progress_bar = keras.callbacks.ProgbarLogger()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define early stopping\n",
    "earlystop = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor=\"val_loss\",\n",
    "    min_delta=0,\n",
    "    patience=50,\n",
    "    verbose=1,\n",
    "    mode=\"auto\",\n",
    "    baseline=None,\n",
    "    restore_best_weights=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "callbacks = [checkpoint, progress_bar, earlystop,]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "411001 406313 817314\n",
      "{0: 1.0057689515225947, 1: 0.9942968508592436}\n"
     ]
    }
   ],
   "source": [
    "bkg_n = np.sum(train_dataset.y[:,1] == 1.0)\n",
    "sig_n = np.sum(train_dataset.y[:,0] == 1.0)\n",
    "total = bkg_n + sig_n\n",
    "print(bkg_n, sig_n, total)\n",
    "weight_for_bkg = (1 / bkg_n) * (total / 2.0)\n",
    "weight_for_sig = (1 / sig_n) * (total / 2.0)\n",
    "class_weights = {0: weight_for_sig, 1: weight_for_bkg}\n",
    "print(class_weights) # dataset is very well balanced but we use weights anyway\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset.shuffle()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Handle high-level data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(817314, 21)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "curr_dataset = train_dataset\n",
    "dnn_vars = [curr_dataset['event_met'], curr_dataset['event_ht'], curr_dataset['event_eta'], curr_dataset['event_m'], curr_dataset['event_MT2'], ]\n",
    "for ii in range(0, 4):\n",
    "    dnn_vars.append(curr_dataset.X['jet'][:, ii, 0])\n",
    "    dnn_vars.append(curr_dataset.X['jet'][:, ii, 1])\n",
    "    dnn_vars.append(curr_dataset.X['jet'][:, ii, 3])\n",
    "    dnn_vars.append(curr_dataset['Dphi'][:, ii])\n",
    "model2_train_X = list(zip(*dnn_vars))\n",
    "model2_train_X = np.array(model2_train_X)\n",
    "model2_train_X.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(350277, 21)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "curr_dataset = val_dataset\n",
    "dnn_vars = [curr_dataset['event_met'], curr_dataset['event_ht'], curr_dataset['event_eta'], curr_dataset['event_m'], curr_dataset['event_MT2'], ]\n",
    "for ii in range(4):\n",
    "    dnn_vars.append(curr_dataset.X['jet'][:, ii, 0])\n",
    "    dnn_vars.append(curr_dataset.X['jet'][:, ii, 1])\n",
    "    dnn_vars.append(curr_dataset.X['jet'][:, ii, 3])\n",
    "    dnn_vars.append(curr_dataset['Dphi'][:, ii])\n",
    "\n",
    "model2_val_X = list(zip(*dnn_vars))\n",
    "model2_val_X = np.array(model2_val_X)\n",
    "model2_val_X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(817314, 250, 5)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset.X['features'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3000\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.74355, saving model to attempt143-8/model_checkpoints/model.001.h5\n",
      "780/780 - 385s - loss: 1.9289 - binary_accuracy: 0.5552 - auc_1: 0.5835 - val_loss: 1.7436 - val_binary_accuracy: 0.6199 - val_auc_1: 0.6745 - 385s/epoch - 493ms/sample\n",
      "Epoch 2/3000\n"
     ]
    }
   ],
   "source": [
    "history = merged_model.fit([model2_train_X, train_dataset.X['points'], train_dataset.X['features'], train_dataset.X['mask']], train_dataset.y,\n",
    "          batch_size=batch_size,\n",
    "          epochs=3000,\n",
    "          validation_data=([model2_val_X, val_dataset.X['points'],  val_dataset.X['features'],  val_dataset.X['mask'],], val_dataset.y),\n",
    "          class_weight=class_weights,\n",
    "          shuffle=True,\n",
    "          verbose=2,                 \n",
    "          callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_model.save(OUTPUT) # save it to disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(history.history['loss'], label='train')\n",
    "plt.plot(history.history['val_loss'], label='val')\n",
    "plt.title('Loss function of the model')\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('loss')\n",
    "plt.legend()\n",
    "plt.savefig('{}/loss.pdf'.format(OUTPUT))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# accuracy\n",
    "fig = plt.figure()\n",
    "plt.plot(history.history['binary_accuracy'], label='train')\n",
    "plt.plot(history.history['val_binary_accuracy'], label='val')\n",
    "plt.title('Accuracy of the model')\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('binary accuracy')\n",
    "plt.legend()\n",
    "plt.hlines(xmin=1, xmax=len(history.history['binary_accuracy']), y=0.8, color='green', linestyle='--')\n",
    "plt.hlines(xmin=1, xmax=len(history.history['binary_accuracy']), y=0.9, color='green', linestyle=':')\n",
    "plt.vlines(ymin=np.min(history.history['val_binary_accuracy']), ymax=0.9, x = len(history.history['val_binary_accuracy'])-51.5, color='red',linestyle=':')\n",
    "plt.savefig('{}/accuracy.pdf'.format(OUTPUT))\n",
    "# plt.show()\n",
    "# plt.savefig('{}/score.pdf'.format(OUTPUT))\n",
    "# history.history['accuracy']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = merged_model.predict([model2_val_X, val_dataset.X['points'], val_dataset.X['features'], val_dataset.X['mask']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = predictions[:, 0]\n",
    "y_true = np.array([1 if v[0]>v[1] else 0 for v in val_dataset.y], dtype=np.single)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title('Predicted score on val set')\n",
    "plt.hist(y_pred[y_true==1], bins=30, histtype='stepfilled', alpha=0.3, label='w300_sq2200')\n",
    "plt.hist(y_pred[y_true==0], bins=30, histtype='stepfilled', alpha=0.3, label='SM')\n",
    "plt.yscale('log')\n",
    "plt.legend()\n",
    "plt.xlabel('score')\n",
    "plt.ylabel(\"MC events\")\n",
    "# plt.show()\n",
    "plt.savefig('{}/score.pdf'.format(OUTPUT))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title('Predicted score on val set (scaled with xs)')\n",
    "sig_score_weights = np.full(y_pred[y_true==1].shape, 3000*sig_xs/y_pred[y_true==1].shape[0])\n",
    "plt.hist(y_pred[y_true==1], bins=30, histtype='stepfilled', weights=sig_score_weights, alpha=0.3, label='w300_sq2200')\n",
    "bkg_score_weights = np.full(y_pred[y_true==0].shape, 3000*bkg_xs/y_pred[y_true==0].shape[0])\n",
    "plt.hist(y_pred[y_true==0], bins=30, histtype='stepfilled', weights=bkg_score_weights, alpha=0.3, label='SM')\n",
    "plt.yscale('log')\n",
    "plt.legend()\n",
    "plt.xlabel('score')\n",
    "plt.ylabel(\"Events for HL-LHC\")\n",
    "# plt.show()\n",
    "plt.savefig('{}/score_norm.pdf'.format(OUTPUT))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert the history.history dict to a pandas DataFrame:     \n",
    "hist_df = pd.DataFrame(history.history) \n",
    "\n",
    "# save to json:  \n",
    "hist_json_file = 'history.json' \n",
    "with open(join(OUTPUT, hist_json_file), mode='w') as f:\n",
    "    hist_df.to_json(f)\n",
    "\n",
    "# or save to csv: \n",
    "hist_csv_file = 'history.csv'\n",
    "with open(join(OUTPUT, hist_csv_file), mode='w') as f:\n",
    "    hist_df.to_csv(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# accuracy\n",
    "fig = plt.figure()\n",
    "plt.plot(history.history['binary_accuracy'], label='train acc', color='blue', linestyle='-')\n",
    "plt.plot(history.history['val_binary_accuracy'], label='val acc', color='blue', linestyle='--')\n",
    "plt.plot(history.history['auc_1'], label='train AUC', color='red', linestyle='-')\n",
    "plt.plot(history.history['val_auc_1'], label='val AUC', color='red', linestyle='--')\n",
    "\n",
    "plt.title('Accuracy of the model')\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('binary accuracy')\n",
    "plt.legend()\n",
    "plt.ylim(0.5, 1.05)\n",
    "plt.vlines(ymin=0.5, ymax=1.05, x = len(history.history['val_binary_accuracy'])-51.5, color='green',linestyle=':')\n",
    "\n",
    "plt.grid()\n",
    "plt.savefig('{}/acc_auc.pdf'.format(OUTPUT))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
